---
title: "Data wrangling"
code-fold: true
---

Data wrangling is the process of making data ready for modelling. It is a tedious task and involves different logical steps.

```{mermaid}
flowchart LR
    Record --> Data --> Tidy\ndata --> Data\nanalysis
    
```

## Core Concepts 

1.  Always look directly into data and see how it look like

2.  Think how it should look like

3.  Think how to rearrange Information to achieve what we want 

4.  Make sure that what we does is doing what we want at every step. 

`r` is an excellent lower level language for data wrangling. The newer `data.table` make it even more faster and intuitive. `data. table` package in `r` is faster than not only tidyverse, it beats most other languages including python. Mainly used for data manipulation, it retains native `data. frame` and all functions written for base r and tidyverse work with data. table. 

eg: `% > %` is called pipe function. It essentially means 'take whatever on the left side and make it the first argument for anything on the right Side'. It comes with tidyverse package. It can also be used with `data.table.` [^1]

[^1]: Now from r 4.1 onwards it is built in to base v as I \>

## data.table

`data.table` looks like `data.frame` superficially but it is lot more efficient, fast and allow numerous actions by simple commands.

```{mermaid}
flowchart TD
data.table --> i
i --> subsetting\nrow
data.table --> j
j --> subsetting\n&\ncalculating\nin\ncolumn
data.table --> by
by --> grouping
data.table --> useful\ncodes
useful\ncodes --> fread
useful\ncodes --> walrus

```

Old `data.frame` allow sub-setting rows and selecting columns. `data.table` which uses the extended syntax

$$
dt[i,j,by]
$$

allow lot more options.

### i,j and by

**`i`**`allows sub-setting and ordering row`

`dt[i]` allow multiple actions at row level of the data. Fundamentally this can be grouped in to sub-setting and ordering.

```{mermaid}
flowchart TD
i --> subsetting
i --> ordering


```

#### Sub-setting

Let us take an example,

We have a data set called `virtual.csv` stored in our computer. The data belongs to a quasi experimental study which compared the effectiveness of virtual class room with respect to physical class room. It also compared the student perception of the two modalities on a likert scale. Study divided students in to two groups, virtual and physical and asked following questions:

-   Is there any difference in the post test score of students in virtual and physical group?

<!-- -->

-   Is there any difference in the student perception in virtual and physical group about the modality of delivery of class?

We can bring it to `r` environment using `fread` command in the data.table library.

``` r
library(data.table)
dt=fread("virtual.csv")
```

```{r}
library(data.table)
dt=fread("virtual.csv")
```

We can see that it contains a set of simple data comparing the performance of medical students in physical classes vs virtual classes and their perceptions.

##### Select only virtual data

Assume, we want to see only the virtual class students. This can be easily achieved by sub-setting the type.class key in the data by "virtual" as shown below.

``` r
library(data.table)
dt=fread("virtual.csv")
dtv=dt[type.class == "virtual"]
head(dtv)
```

```{r}
#| collapse: true
library(data.table)
dt=fread("virtual.csv")
dtv=dt[type.class == "virtual"]
head(dtv)
```

##### Select first 2 rows

If we want to select only first 2 rows[^2] , us the following code.

[^2]: note that, \[1:2,\] is not necessary. However \[1:2,\] shows the full picture.

    > Select first 2 rows and retain all the columns

``` r
dt2=dt[1:2]
dt2
```

```{r}
#| collapse: true
dt2=dt[1:2]
dt2
```

##### Ordering

Variables in the rows can be arranged in ascending or descending order by using the `order()` command as shown below.

``` r
dta=dt[order(posttest.score)] # use -posttest.score for descending order
dta
```

```{r}
#| collapse: true
dta=dt[order(posttest.score)] 
# use -posttest.score for descending order
dta
```

**`j`**`allows manipulations in the column`

dt\[,j\] allow selection, correction or mutation and calculations at column level.

```{mermaid}
flowchart TD
j --> Select
j --> Calculation
j --> Correct\ntypo
j --> Add\nor\nDelete\ncolumn

```

##### Select specific column[^3]

[^3]: dt\[,.(j)\]: blank followed by , denotes selecting all rows. "." denote "list". if "`."`is not used, data.table will give the output as vectors, instead of list.

Suppose we want to select only the necessary columns from our `dt` data set. We can do this by simple code as shown below.

``` r
dtc=dt[,.(idertifier,type.class,pretest.score,posttest.score,sex,likert.total)]
dtc
```

```{r}
#| collapse: true
dtc=dt[,.(idertifier,type.class,pretest.score,posttest.score,sex,likert.total)]
dtc
```

`data.table`‘s `j` can handle more than just *selecting columns* - it can handle *expressions*, i.e., *computing on columns*. 

##### Correct typos

In the dataset dt, we can see that the first key, identifier is misspelt as idertifier. We can correct it by following code

``` r
dtc=dt[,.(identifier=idertifier,type.class,pretest.score,posttest.score,sex,likert.total)]
dtc
```

```{r}
#| collapse: true
dtc=dt[,.(identifier=idertifier,type.class,pretest.score,posttest.score,sex,likert.total)]
dtc
```

##### Computing on columns

To calculate the mean post test score of the virtual class, following code can be used.

``` r
dtm=dtc[type.class=="virtual",mean(posttest.score)]
dtm
```

```{r}
#| collapse: true
dtm=dtc[type.class=="virtual",mean(posttest.score)]
dtm
```

##### Add or delete column

If we want to add mean post test score of virtual on to dtv.

To create or delete column, we use `:=`, called walrus operator.[^4]

[^4]: The walrus is a large pinniped marine mammal with discontinuous distribution about the North Pole in the Arctic Ocean and subarctic seas of the Northern Hemisphere. 

``` r
dtvm=dtc[,mean.pt.score:=mean(posttest.score)]
dtvm
```

```{r}
#| collapse: true
dtvm=dtc[,mean.pt.score:=mean(posttest.score)]
head(dtvm)
```

To delete mean.pt.score,

``` r
dtvn=dtvm[,mean.pt.score:=NULL]
dtvn
```

```{r}
#| collapse: true
dtvn=dtvm[,mean.pt.score:=NULL]
head(dtvn)
```

**`by`**`allow grouping`

```{mermaid}
flowchart LR
by --> grouping
```

### Aggregation

To group or aggregate data based on a `key`, the `by` operator can be used.

``` r
dtmn=dt[,mean(posttest.score), by=.(type.class)]
dtmn
```

```{r}
#| collapse: true
dtmn=dt[,mean(posttest.score), by=.(type.class)]
dtmn
```

`data.table` allow fast reading of data in to data table

```{mermaid}
flowchart TD
  A[multiple data tables] --> B(map)
  B --> C(rbind or rbindlist)
  C --> D[Stack one on top of another column]
  A --> E[merge]
  E --> F(attach raws to the side of raws)
```

### rbind

`rbind ( )` stacks data variables on top of one another from different data. tables. For this to work all the identifiers should be same on the different files that has to be compiled.

``` r
#| collapse: true
library(readxl)
Jan <- read_excel("Jan.xlsx")
jandt=as.data.table(Jan)
str(jandt)

Feb <- read_excel("Feb.xlsx")
febdt=as.data.table(Feb)
str(febdt)
```

```{r}
#| collapse: true
library(readxl)
Jan <- read_excel("Jan.xlsx")
jandt=as.data.table(Jan)
str(jandt)

Feb <- read_excel("Feb.xlsx")
febdt=as.data.table(Feb)
str(febdt)

```

``` r
#| collapse: true
rbind(jandt,febdt)
```

```{r}
#| collapse: true
rbind(jandt,febdt)
```

#### merge

```{mermaid}
flowchart TD
  A[merge] --> B(outer join)
  B --> C(use all=TRUE)
  A --> D[left join]
  D --> E[use all.x=TRUE]
  A --> F(right join)
  F --> G(use all.y=TRUE)
 
```

We can merge the data tables using `merge ( )` function, if there is at-least one common identifier in each row.

eg: Assume we have two data tables with a common identifier ID.

```{r}
#| collapse: true
# Load the data.table package
library(data.table)

# Create the first data table with names and ages
dt1 <- data.table(
  id=c("1","2","3","4","5"),
  Name = c("Scut", "Sambu", "Shibu", "Diana", "Echu"),
  Age = c(28, 34, 23, 41, 30)
)

# Create the second data table with sex and occupation
dt2 <- data.table(
  id=c("1","2","3","4","5"),
  Sex = c("Female", "Male", "Male", "Female", "Male"),
  Occupation = c("Fraudster", "Doctor", "Artist", "Teacher", "Chef")
)

# Print the data tables to verify
print(dt1)
print(dt2)
```

Now we can merge dt1 and dt2 using `merge( )` function

```{r}
#|collapse: TRUE
dtf=merge.data.table(dt1,dt2, by="id", all = TRUE)
head(dtf)
```

## readxl() and map()

If data is in multiple excel files, use `readxl( )`, `map ( )` and `rbindlist ( )` to compile data.

`map ( )` comes with `purr` package.

```{r warning=FALSE}
#| collapse: true
library(readxl)
library(purrr)

filelist=list.files(path="/Users/drkmenon/Sync/knotesquarto/Biomedical/Research Methodology",pattern='xlsx',full.names = TRUE)


process.file=function(df) {
   sales=df[1:5,4]
   employee=df[1:5,3]
   return(data.table(sales=sales,employee=employee))
}
   

compiled.data=filelist |>
  map(read_excel) |>
  map(process.file) |>
  rbindlist()

compiled.data

```

## Data to tidy data

A tidy data should satisfy following criteria

-   Each variable forms a column

-   Each observation should form a row

-   Each observation unit forms a table

In tidy data there are two types of data points

-   Identifying variable (key)

-   Measures/Values

eg:

| ID  | Person | Points | Consumption |
|-----|--------|--------|-------------|
| 1   | scut   | 1      | 100         |
| 2   | sambu  | 30     | 10          |
| 3   | shibu  | 50     | 5           |

: An example tidy data table

## Untidy data

Following data is not so tidy

| ID  | Religion | 10-20k | 20-30k | \>30k |
|-----|----------|--------|--------|-------|
| Jew | Jew      | 100    | 20     | 10    |
| Chr | Chr      | 200    | 40     | 15    |
| Hin | Hin      | 2      | 30     | 500   |

: Not so tidy data

### How to tidy data

`melt ( )` to pivot long and `dcast( )` for pivot wide

::: callout-note
### melt

It takes a single row of N columns and turn it in to N rows in a single column, using keys
:::

`melt ( )` asks for

-   data.table

-   id.vars

-   measure.vars (Column names)

-   variable.name (new name)

-   value.name (new name)

#### Syntax

``` r
df |>

melt(measure.vars=patterns='^xyz',
     variable.name= "",
     value.name='')
```

```{r}
#| collapse: true
wd=data.table(
  ID=c(1,2,3),
  Religion=c("jew","chr","hin"),
  "10-20k"=c(100,200,2),
  "20-30k"=c(20,40,30),
  ">30k"=c(10,15,500)
)
head(wd)
```

#### Make data.table "wd" long

```{r}
#| collapse: true
wd |>
melt(measure.vars = patterns('k'),
    variable.name = "Income",
    value.name = "Number"
  )
```

### Adding data to r and reading it as data.table

We may once more visit how to add data to `r` using data table and try to do simple analysis

``` r
library(data.table)
dt=fread("virtual.csv")
```

```{r}
#| collapse: true
library(data.table)
dt=fread("virtual.csv")
```

### How to look at data

#### vtable

`vtable` is a package that helps to display the structure better than other commands.

``` r
#| collapse: true
vtable::vt(dt)
```

```{r}
#| collapse: true
vtable::vt(dt)
```

#### Other methods

-   str(dt) 

    ```{r}
    #| collapse: true
    str(dt)
    ```

-   tables(dt)

-   summary(dt) 

### Stages of data wrangling

Having added the data and looked in to the structure, me have to go through 3 stages before we can do analysis especially if the record is not structured properly

-   from record to data

-   data to tidy data

-   from tidy data to data analysis. 

### Process of wrangling

#### From records to data

::: callout-note
## Record

'Raw source', not in workable format. 
:::

#### Create a data.table

virtual.csv has multiple columns, of which we require only the following:

``` r
fdt=dt[,.(identifier=as.factor(idertifier),type.class=as.factor(type.class),posttest.score=as.numeric(posttest.score),pretest.score=as.numeric(pretest.score),total.likert=as.numeric(likert.total), sex=as.factor(sex))]
head(fdt)
```

Here we can see that using `j`, we have sub-setted, renamed and fixed the type of data

```{r}
#| collapse: true
fdt=dt[,.(identifier=as.factor(idertifier),type.class=as.factor(type.class),posttest.score=as.numeric(posttest.score),pretest.score=as.numeric(pretest.score),total.likert=as.numeric(likert.total), sex=as.factor(sex))]
head(fdt)
```

In our study, the outcome variables are `posttest.score` and `total.likert`. The grouping variable is `type.class`.

### Summary of data

```{r}
#| collapse: true
summary(fdt)
attach(fdt)
mean.score= fdt[,mean(posttest.score),by=type.class]
mean.pre.score= fdt[,mean(pretest.score),by=type.class]
mean.likert= fdt[,mean(total.likert),by=type.class]
mean.pre.score
mean.score
mean.likert
boxplot(posttest.score~type.class, main="Box plot showing Post test score vs type of class")
boxplot(pretest.score~type.class,main="Box plot showing Pre test score vs type of class")
boxplot(total.likert~type.class, main="Box plot showing likert score vs type of class")
boxplot(total.likert~sex, main="Box plot showing likert score vs sex")
```

### Check for normality visually

``` r
hist(posttest.score, main = "",breaks = 15)
hist(total.likert, main="",breaks = 10)
```

```{r}
#| collapse: true
hist(posttest.score, main = "Histogram showing non normal distribution of post test score",breaks = 15)
hist(total.likert, main="Histogram showing doubtful normal distribution of likert score",breaks = 10)

```

Here we can see that both posttest.score and total.likert are visually not normally distributed. He may cross check it by doing shapiro test

### Test for normality

``` r
shapiro.test(posttest.score)
```

```{r}
#| collapse: true
shapiro.test(posttest.score)
```

a `p value` of \<.005 suggest that posttest score is significantly not normal.

### Welch t test

As data is not normal, we have to do modified t test.

``` r
t.test(posttest.score,pretest.score,var.equal=FALSE)
```

```{r}
#| collapse: true
t.test(posttest.score,pretest.score,var.equal=FALSE)
```

``` r
t.test(posttest.score~type.class,var.equal=FALSE)
```

```{r}
#| collapse: true
t.test(posttest.score~type.class,var.equal=FALSE)
```

### Shapiro test for total.likert

``` r
shapiro.test(total.likert)
```

```{r}
#| collapse: true
shapiro.test(total.likert)
```

Here the shapiro test is not significant. So the data can be taken as normal.

``` r
t.test(total.likert~type.class)
```

```{r}
#| collapse: true
t.test(total.likert~type.class)
```
