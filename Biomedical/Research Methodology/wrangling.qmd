---
title: "Data wrangling"
code-fold: true
---

Data wrangling is the process of making data ready for modelling. It is a tedious task and involves different logical steps.

```{mermaid}
flowchart LR
    Record --> Data --> Tidy\ndata --> Data\nanalysis
    
```

## Core Concepts 

1.  Always look directly into data and see how it look like

2.  Think how it should look like

3.  Think how to rearrange Information to achieve what we want 

4.  Make sure that what we does is doing what we want at every step. 

`r` is an excellent lower level language for data wrangling. The newer `data.table` make it even more faster and intuitive. `data. table` package in `r` is faster than not only tidyverse, it beats most other languages including python. Mainly used for data manipulation, it retains native `data. frame` and all functions written for base r and tidyverse work with data. table. 

eg: `% > %` is called pipe function. It essentially means 'take whatever on the left side and make it the first argument for anything on the right Side'. It comes with tidyverse package. It can also be used with `data.table.` [^1]

[^1]: Now from r 4.1 onwards it is built in to base v as I \>

## data.table

`data.table` looks like `data.frame` superficially but it is lot more efficient, fast and allow numerous actions by simple commands.

```{mermaid}
flowchart TD
data.table --> i
i --> subsetting\nrow
data.table --> j
j --> subsetting\n&\ncalculating\nin\ncolumn
data.table --> by
by --> grouping
data.table --> useful\ncodes
useful\ncodes --> fread
useful\ncodes --> walrus

```

Old `data.frame` allow sub-setting rows and selecting columns. `data.table` which uses the extended syntax

$$
dt[i,j,by]
$$

allow lot more options.

### i,j and by

**`i`**`allows sub-setting and ordering row`

`dt[i]` allow multiple actions at row level of the data. Fundamentally this can be grouped in to sub-setting and ordering.

```{mermaid}
flowchart TD
i --> subsetting
i --> ordering


```

#### Sub-setting

Let us take an example,

We have a data set called `virtual.csv` stored in our computer. The data belongs to a quasi experimental study which compared the effectiveness of virtual class room with respect to physical class room. It also compared the student perception of the two modalities on a likert scale. Study divided students in to two groups, virtual and physical and asked following questions:

-   Is there any difference in the post test score of students in virtual and physical group?

```{=html}
<!-- -->
```
-   Is there any difference in the student perception in virtual and physical group about the modality of delivery of class?

We can bring it to `r` environment using `fread` command in the data.table library.

``` r
library(data.table)
dt=fread("virtual.csv")
```

```{r}
library(data.table)
dt=fread("virtual.csv")
```

We can see that it contains a set of simple data comparing the performance of medical students in physical classes vs virtual classes and their perceptions.

##### Select only virtual data

Assume, we want to see only the virtual class students. This can be easily achieved by sub-setting the type.class key in the data by "virtual" as shown below.

``` r
library(data.table)
dt=fread("virtual.csv")
dtv=dt[type.class == "virtual"]
head(dtv)
```

```{r}
#| collapse: true
library(data.table)
dt=fread("virtual.csv")
dtv=dt[type.class == "virtual"]
head(dtv)
```

##### Select first 2 rows

If we want to select only first 2 rows[^2] , us the following code.

[^2]: note that, \[1:2,\] is not necessary. However \[1:2,\] shows the full picture.

    > Select first 2 rows and retain all the columns

``` r
dt2=dt[1:2]
dt2
```

```{r}
#| collapse: true
dt2=dt[1:2]
dt2
```

##### Ordering

Variables in the rows can be arranged in ascending or descending order by using the `order()` command as shown below.

``` r
dta=dt[order(posttest.score)] # use -posttest.score for descending order
dta
```

```{r}
#| collapse: true
dta=dt[order(posttest.score)] 
# use -posttest.score for descending order
dta
```

**`j`**`allows manipulations in the column`

dt\[,j\] allow selection, correction or mutation and calculations at column level.

```{mermaid}
flowchart TD
j --> Select
j --> Calculation
j --> Correct\ntypo
j --> Add\nor\nDelete\ncolumn

```

##### Select specific column[^3]

[^3]: dt\[,.(j)\]: blank followed by , denotes selecting all rows. "." denote "list". if "`."`is not used, data.table will give the output as vectors, instead of list.

Suppose we want to select only the necessary columns from our `dt` data set. We can do this by simple code as shown below.

``` r
dtc=dt[,.(idertifier,type.class,pretest.score,posttest.score,sex,likert.total)]
dtc
```

```{r}
#| collapse: true
dtc=dt[,.(idertifier,type.class,pretest.score,posttest.score,sex,likert.total)]
dtc
```

`data.table`‘s `j` can handle more than just *selecting columns* - it can handle *expressions*, i.e., *computing on columns*. 

##### Correct typos

In the dataset dt, we can see that the first key, identifier is misspelt as idertifier. We can correct it by following code

``` r
dtc=dt[,.(identifier=idertifier,type.class,pretest.score,posttest.score,sex,likert.total)]
dtc
```

```{r}
#| collapse: true
dtc=dt[,.(identifier=idertifier,type.class,pretest.score,posttest.score,sex,likert.total)]
dtc
```

##### Computing on columns

To calculate the mean post test score of the virtual class, following code can be used.

``` r
dtm=dtc[type.class=="virtual",mean(posttest.score)]
dtm
```

```{r}
#| collapse: true
dtm=dtc[type.class=="virtual",mean(posttest.score)]
dtm
```

##### Add or delete column

If we want to add mean post test score of virtual on to dtv.

To create or delete column, we use `:=`, called walrus operator.[^4]

[^4]: The walrus is a large pinniped marine mammal with discontinuous distribution about the North Pole in the Arctic Ocean and subarctic seas of the Northern Hemisphere. 

``` r
dtvm=dtc[,mean.pt.score:=mean(posttest.score)]
dtvm
```

```{r}
#| collapse: true
dtvm=dtc[,mean.pt.score:=mean(posttest.score)]
head(dtvm)
```

To delete mean.pt.score,

``` r
dtvn=dtvm[,mean.pt.score:=NULL]
dtvn
```

```{r}
#| collapse: true
dtvn=dtvm[,mean.pt.score:=NULL]
head(dtvn)
```

**`by`**`allow grouping`

```{mermaid}
flowchart LR
by --> grouping
```

### Aggregation

To group or aggregate data based on a `key`, the `by` operator can be used.

``` r
dtmn=dt[,mean(posttest.score), by=.(type.class)]
dtmn
```

```{r}
#| collapse: true
dtmn=dt[,mean(posttest.score), by=.(type.class)]
dtmn
```

`data.table` allow fast reading of data in to data table

### Adding data to r and reading it as data.table

We may once more visit how to add data to `r` using data table and try to do simple analysis

``` r
library(data.table)
dt=fread("virtual.csv")
```

```{r}
#| collapse: true
library(data.table)
dt=fread("virtual.csv")
```

### How to look at data

#### vtable

`vtable` is a package that helps to display the structure better than other commands.

``` r
#| collapse: true
vtable::vt(dt)
```

```{r}
#| collapse: true
vtable::vt(dt)
```

#### Other methods

-   str(dt) 

    ```{r}
    #| collapse: true
    str(dt)
    ```

-   tables(dt)

-   summary(dt) 

### Stages of data wrangling

Having added the data and looked in to the structure, me have to go through 3 stages before we can do analysis especially if the record is not structured properly

-   from record to data

-   data to tidy data

-   from tidy data to data analysis. 

### Process of wrangling

#### From records to data

::: callout-note
## Record

'Raw source', not in workable format. 
:::

```{mermaid}
flowchart TD
  A[multiple data tables] --> B(map)
  B --> C(rbind or rbindlist)
  C --> D[Stack one on top of another column]
  A --> E[merge]
  E --> F(attach raws to the side of raws)
```

### rbind

`rbind ( )` stacks data variables on top of one another from different data. tables. For this to work all the identifiers should be same on the different files that has to be compiled.

``` r
#| collapse: true
library(readxl)
Jan <- read_excel("Jan.xlsx")
jandt=as.data.table(Jan)
str(jandt)

Feb <- read_excel("Feb.xlsx")
febdt=as.data.table(Feb)
str(febdt)
```

```{r}
#| collapse: true
library(readxl)
Jan <- read_excel("Jan.xlsx")
jandt=as.data.table(Jan)
str(jandt)

Feb <- read_excel("Feb.xlsx")
febdt=as.data.table(Feb)
str(febdt)

```

``` r
#| collapse: true
rbind(jandt,febdt)
```

```{r}
#| collapse: true
rbind(jandt,febdt)
```

#### merge

We can merge the data tables using `merge ( )` function, if there is atleast one common identifier in each row.

```{r}
fdt=dt[,.(identifier=as.factor(idertifier),type.class=as.factor(type.class),posttest.score=as.numeric(posttest.score),pretest.score=as.numeric(pretest.score),total.likert=as.numeric(likert.total), sex=as.factor(sex))]
head(fdt)
```

```{r}
summary(fdt)
attach(fdt)
mean.score= fdt[,mean(posttest.score),by=type.class]
mean.pre.score= fdt[,mean(pretest.score),by=type.class]
mean.likert= fdt[,mean(total.likert),by=type.class]
mean.pre.score
mean.score
mean.likert
boxplot(posttest.score~type.class)
boxplot(pretest.score~type.class)
boxplot(total.likert~type.class)
boxplot(total.likert~sex)
```

```{r}
shapiro.test(posttest.score)
```

```{r}
t.test(posttest.score,pretest.score,var.equal=FALSE)
```

```{r}
t.test(posttest.score~type.class,var.equal=FALSE)
```

```{r}
shapiro.test(total.likert)
```

```{r}
t.test(total.likert~type.class, var.equal=FALSE)
```

## readxl() and map()

If data is in multiple excel files, use `readxl( )`, `map ( )` and `rbindlist ( )` to compile data.

`map ( )` comes with `purr` package.

```{r}
library(readxl)
library(purrr)

filelist=list.files(path="/Users/drkmenon/Sync/knotesquarto/Biomedical/Research Methodology",pattern='xlsx',full.names = TRUE)


process.file=function(df) {
   sales=df[1:5,4]
   employee=df[1:5,3]
   return(data.table(sales=sales,employee=employee))
}
   

compiled.data=filelist |>
  map(read_excel) |>
  map(process.file) |>
  rbindlist()

compiled.data

```
