{
  "hash": "8a2e6937d1ee4baf8b421d841ea0d61c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Draw-downs and buildups\"\ncode-fold: true\nnosite: |\n  @bernoulli1954\norder: 5\n---\n\n\n\n\n## Skewed and fat tailed\n\n![A skewed distribution with fat tails. Tail is truncated on left side at zero](images/skewed.png)\n\n### Skewed\n\nA distribution is called left skewed if it has got a long tail on the left side. This means most of the data points are concentrated on the right side. But there are `a number of data points` that are significantly lower than the bulk of the values.\n\nA distribution is called right skewed if it has got a long tail on the right side. This means most of the data points are concentrated on the left side. But there are `a number of data points` that are significantly higher than the bulk of the values.\n\nùú∑, the skewness parameter is used to define this.\n\n| ùú∑    | Skewness          |\n|------|-------------------|\n| \\< 0 | Negatively skewed |\n| 0    | Symmetric         |\n| \\>0  | Positively skewed |\n\n: Skewness and ùú∑\n\n## Fat tail\n\nIn a normal distribution, data points on either ends of the distribution are not only symmetric, but they significantly thin out while we move away from the central location. This means that we can ignore these tails on either ends and reach meaningful conclusions with out significant departure from the reality.\n\nBut in case of fat tailed distributions, the data points in the tails significantly alter the reality if they are ignored.\n\n‚ç∫, the stability factor determines this tail behaviour. It lies between 0 and 2.\n\nAn ‚ç∫ of 2 represents a normal distribution with very thin tails.\n\n::: callout\nAs ‚ç∫ moves away from 2 (towards 0), the fatness of the tail increases. That means there are more number of data points in the tail\n:::\n\n## Levy distribution\n\nIt is a kind of distribution where the ‚ç∫ is moved away from 2, meaning, **fat tailed** and ùú∑ is moving away from 0. That means, it can be either left or right skewed.\n\n## Left skewed Levy distribution\n\nWe are particularly interested in Levy distribution while analysing returns from a volatile instrument.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load the necessary package\n# install.packages(\"stabledist\")\nlibrary(stabledist)\n\n# Parameters for the L√©vy distribution\nalpha <- 1.505  # Stability parameter\nbeta <- -0.084 # Negative skewness parameter (left-skewed)\ngamma <- 0.006433654  # Scale parameter\ndelta <- 0.0007449581 # Location parameter\n\n# Number of simulations\nn <- 150\n\n# Generate random numbers from the L√©vy distribution\nset.seed(123) # For reproducibility\nlevy_data <- rstable(n, alpha, beta, gamma, delta)\n\n# Plot the histogram of the simulated data\nhist(levy_data, breaks = 50, main = \"Left-Skewed L√©vy Distribution\", xlab = \"Data\", ylab = \"Frequency\", col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](levi_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\nIf we analyse the Levy distribution shown in the picture above, we can conclude on following points:\n\n-   Most of the data are clustered around the central point, ùõø.\n\n-   There are no values on the right side of ùõø which are way to far away.\n\n-   There are data points on the left side of ùõø which are significantly far away.\n\n-   Their frequency is not negligible, considering the sample size of just 150.\n\n-   Gains are limited and stays closer to ùõø.\n\n-   `Losses are often significantly huge.`\n\n-   `Frequency of huge loss is more than huge gains.`\n\n`But in case of standard volatile instruments, the left side is truncated at -100%`\n\n## 1000 alternate worlds\n\nIf we imagine 1000 alternative worlds were this distribution is at play, we can see that each world will be experiencing different returns.\n\n![](images/notsogoo.png){width=\"60\"}\n\nSome will be experiencing repeated huge draw-downs followed by not so huge build-ups.\n\nOthers may experience significant draw-downs followed by multiple towering build-ups.\n\n![](images/huge.png){width=\"84\"}\n\nWhat is common to all these worlds are frequent and extreme draw-downs.\n\n## What is the probability of various return rates?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stabledist)\n\n# Simulation parameters\nset.seed(233)\nalpha <- 1.505  # Stability parameter\nbeta <- -0.084 # Negative skewness parameter (left-skewed)\ngamma <- 0.006433654  # Scale parameter\ndelta <- 0.0007449581 # Location parameter\n\nn <- 10000  # Number of simulations\n\n# Generate samples from a stable distribution\nlevy_samples <- rstable(n, alpha, beta, gamma, delta)  \n\n# Truncate returns at -100%\ntruncated_samples <- pmax(levy_samples, -100)\n\n# Adjust mean and standard deviation (Historical)\ndesired_mean <- 10.8\ndesired_sd <- 21\n\n# Calculate current mean and standard deviation\ncurrent_mean <- mean(truncated_samples)\ncurrent_sd <- sd(truncated_samples)\n\n# Apply linear transformation to adjust mean and standard deviation\nadjusted_samples <- (truncated_samples - current_mean) * (desired_sd / current_sd) + desired_mean\n\n# Ensure minimum value is -100 after truncation\nadjusted_samples <- pmax(adjusted_samples, -100)\n\n# Example: Simulate future returns over a period\nfuture_returns <- adjusted_samples\n\n# Basic summary statistics\ncat(\"Summary Statistics:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSummary Statistics:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(summary(future_returns))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-100.000    8.271   11.365   11.146   14.424  252.402 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Basic histogram\nhist(future_returns, breaks = 50, main = \"Histogram of Simulated Returns\", xlab = \"Returns\")\n```\n\n::: {.cell-output-display}\n![](levi_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Probability calculations\np_return_greater_15 <- mean(future_returns > 15)\ncat(paste(\"P(Return > 15) =\", p_return_greater_15, \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(Return > 15) = 0.213 \n```\n\n\n:::\n\n```{.r .cell-code}\np_return_greater_30 <- mean(future_returns > 30)\ncat(paste(\"P(Return > 30) =\", p_return_greater_30, \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(Return > 30) = 0.0134 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Additional probability calculations\np_between_5_and_15 <- mean(future_returns > 5 & future_returns <= 15)\ncat(paste(\"P(5 < Return <= 15) =\", p_between_5_and_15, \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(5 < Return <= 15) = 0.6792 \n```\n\n\n:::\n\n```{.r .cell-code}\np_between_15_and_30 <- mean(future_returns > 15 & future_returns <= 30)\ncat(paste(\"P(15 < Return <= 30) =\", p_between_15_and_30, \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(15 < Return <= 30) = 0.1996 \n```\n\n\n:::\n\n```{.r .cell-code}\np_between_neg5_and_5 <- mean(future_returns > -5 & future_returns <= 5)\ncat(paste(\"P(-5 <= Return <= 5) =\", p_between_neg5_and_5, \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(-5 <= Return <= 5) = 0.0849 \n```\n\n\n:::\n\n```{.r .cell-code}\np_between_neg15_and_neg5 <- mean(future_returns > -15 & future_returns <= -5)\ncat(paste(\"P(-15 <= Return <= -5) =\", p_between_neg15_and_neg5, \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(-15 <= Return <= -5) = 0.0126 \n```\n\n\n:::\n\n```{.r .cell-code}\np_less_neg15 <- mean(future_returns < -15)\ncat(paste(\"P(Return < -15) =\", p_less_neg15, \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(Return < -15) = 0.0103 \n```\n\n\n:::\n\n```{.r .cell-code}\np_less_neg30 <- mean(future_returns < -30)\ncat(paste(\"P(Return < -30) =\", p_less_neg30, \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(Return < -30) = 0.0057 \n```\n\n\n:::\n\n```{.r .cell-code}\np_less_neg60 <- mean(future_returns < -60)\ncat(paste(\"P(Return < -60) =\", p_less_neg60, \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(Return < -60) = 0.0023 \n```\n\n\n:::\n:::\n\n\n\n\n### Summary\n\n**The simulated returns exhibit the following characteristics:**\n\n-   **Minimum**: The lowest return observed after truncation is -100, indicating that no return falls below this threshold due to truncation.\n\n-   **1st Quartile**: 25% of the returns fall below this value.\n\n-   **Median**: 50% of the returns fall below this value, indicating the central tendency of the distribution.\n\n-   **Mean**: The average return is approximately 11.146, representing the central tendency of the distribution.\n\n-   **3rd Quartile**: 75% of the returns fall below this value.\n\n-   **Maximum**: The highest return observed is 252.402, indicating that no return exceeds this threshold.\n\nThe probability calculations reveal the likelihood of observing returns within specific ranges:\n\n-   There is a 21.3% chance that the return will exceed 15.\n\n-   There is a 1.34% chance that the return will exceed 30.\n\n-   There is a 67.92% chance that the return will fall between 5 and 15.\n\n-   There is a 19.96% chance that the return will fall between 15 and 30.\n\n-   There is an 8.49% chance that the return will fall between -5 and 5.\n\n-   There is a 1.26% chance that the return will fall between -15 and -5.\n\n-   There is a 1.03% chance that the return will be less than -15.\n\n-   There is a 0.57% chance that the return will be less than -30.\n\n-   There is a 0.23% chance that the return will be less than -60.\n\n## Value at Risk (VaR) and Conditional VaR\n\nTraditionally, mean and the variance are used as metrics to select an appropriate portfolio. One down side of mean-variance-risk assessment is that it assumes a symmetric distribution and secondly proposes to truncate variance on either side to reduce risk.\n\nIn fact variance is beneficial for buildup of the right side of the distribution and adds convexity to the benefit if we can mitigate the risk of large draw-downs on the left side.\n\nTwo metrics that specifically looks for tail risk are Value at Risk (VaR) and Conditional Value at Risk (CVaR)\n\n### VaR\n\nThis measures the maximum loss over a given time period with a certain confidence level. For example, VaR at a 95% confidence level is the loss that will not be exceeded with 95% probability.\n\n### CVaR\n\nThis measures the expected loss given that the loss has exceeded the VaR threshold. It's also known as Expected Shortfall.\n\nCVaR is the most useful measure to foresee the shortfall in the capital, if the tail risk remains unattended.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Confidence level\nconfidence_level <- 0.95\n\n# Calculate VaR (95% confidence level)\nVaR_95 <- quantile(future_returns, 1 - confidence_level)\n\n# Calculate CVaR (Expected Shortfall)\nCVaR_95 <- mean(future_returns[future_returns <= VaR_95])\n\n# Calculate maximum CVaR (worst loss within the VaR threshold)\nmax_CVaR_95 <- min(future_returns[future_returns <= VaR_95])\n\n# Adjust for the investment amount (assuming VaR and CVaR are in percentage terms)\ninvestment <- 100\nVaR_95_rupees <- investment * (VaR_95 / 100)\nCVaR_95_rupees <- investment * (CVaR_95 / 100)\nmax_CVaR_95_rupees <- investment * (max_CVaR_95 / 100)\n\n# Print results rounded to 2 decimal places\ncat(\"VaR (95% confidence level) =\", round(VaR_95_rupees, 2), \"rupees\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVaR (95% confidence level) = 1.5 rupees\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"CVaR (95% confidence level) =\", round(CVaR_95_rupees, 2), \"rupees\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCVaR (95% confidence level) = -11.61 rupees\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Maximum CVaR (95% confidence level) =\", round(max_CVaR_95_rupees, 2), \"rupees\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMaximum CVaR (95% confidence level) = -100 rupees\n```\n\n\n:::\n:::\n\n\n\n\nIn this scenario, we can see that VaR and CVaR are reasonably managable. We can say with in 95 % of scenarios, loss well not exeed ‚Çπ1.5 and in 5% scenarios where it exeeds ‚Çπ1.5, average loss will be aroud ‚Çπ11.61, for every ‚Çπ100 invested.\n\nImportant catch is that in those 5% rare scenarios, the maximum CVaR can reach upto ‚Çπ100, that is the entire corpus can be washed out.\n\nAs Bernoulli would insist\n\n-   Larger the part of fortune greater the imprudence of the gambler in a game of chance\n\n-   A man who risks his entire fortune acts like a simpleton, however great may be the possible gain\n\n-   Either take insurance or keep a significant portion away from the game of chance\n\n-   It is advisable to divide the fortune which are exposed to chance in to several portions rather than to risk them all together\n\nThis take us to the logical next step, how to [mitigate the tail risk](https://www.knotes.co.in/catallactics/personal/speculation)\n",
    "supporting": [
      "levi_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}